{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24e40fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "from torchvision import  models,transforms\n",
    "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "087d70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteCrackDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = {\"Non-cracked\": 0, \"Cracked\": 1}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label_str = self.annotations.iloc[idx, 1]\n",
    "        \n",
    "        # Handle case sensitivity and strip any whitespace\n",
    "        label_str = label_str.strip()\n",
    "        if label_str.lower() == \"non-cracked\" or label_str.lower() == \"non-crack\" or label_str.lower() == \"non-cracked\":\n",
    "            label = 0\n",
    "        elif label_str.lower() == \"cracked\" or label_str.lower() == \"crack\":\n",
    "            label = 1\n",
    "        else:\n",
    "            print(f\"Unknown label: {label_str}, defaulting to Non-cracked (0)\")\n",
    "            label = 0\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c61237cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained MobileNetV3 model\n",
    "def get_MobileNetV3_model(num_classes=2):\n",
    "\n",
    "    weights = MobileNet_V3_Large_Weights.DEFAULT\n",
    "    model = mobilenet_v3_large(weights=weights)\n",
    "    model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77063920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Update statistics\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Store predictions and targets for additional metrics\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='binary')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": torch.tensor(accuracy),\n",
    "        \"precision\": torch.tensor(precision),\n",
    "        \"recall\": torch.tensor(recall),\n",
    "        \"f1\": torch.tensor(f1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3210508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        try:\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Print loss occasionally\n",
    "            if num_batches % 50 == 0:\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f\"Batch {num_batches}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    if num_batches == 0:\n",
    "        print(\"WARNING: No valid batches in this epoch!\")\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f\"Processed {num_batches} batches\")\n",
    "    print(f\"Training Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a1ba79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Train for one epoch\n",
    "        epoch_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        val_accuracy = val_metrics['accuracy'].item()\n",
    "        val_f1 = val_metrics['f1'].item()\n",
    "        \n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%, F1-Score: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model based on accuracy\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            torch.save(best_model_wts, f'best_model_epoch_{epoch+1}.pth')\n",
    "            print(f\"Saved best model with Accuracy: {best_accuracy:.2f}%\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "294530c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # MobileNetV3 typically uses 224x224\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ConcreteCrackDataset('../artifact_folder/train/labels.csv', '../artifact_folder/train/images', transform)\n",
    "    val_dataset = ConcreteCrackDataset('../artifact_folder/val/labels.csv', '../artifact_folder/val/images', transform)\n",
    "    test_dataset = ConcreteCrackDataset('../artifact_folder/test/labels.csv', '../artifact_folder/test/images', transform)\n",
    "    \n",
    "    # Check if datasets are loaded correctly\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    print(f\"----------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Verify a few samples to ensure labels are valid\n",
    "    print(\"Checking a few samples from training set:\")\n",
    "    for i in range(min(3, len(train_dataset))):\n",
    "        img, label = train_dataset[i]\n",
    "        print(f\"Sample {i} - Image shape: {img.shape}, Label: {label}\")\n",
    "\n",
    "    # Use a batch size suitable for MobileNetV3\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    # Get MobileNetV3 model\n",
    "    model = get_MobileNetV3_model(num_classes=2)  # 2 classes: Non-cracked (0) / Cracked (1)\n",
    "    model.to(device)\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    # Use learning rate scheduler to reduce lr by 0.1 every 3 epochs\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    print(f\"----------------------------------------------------------------------------\")\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    model, best_accuracy = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=10)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.2f}%\")\n",
    "    torch.save(model.state_dict(), 'MobileNetV3_concrete_crack_detector.pth')\n",
    "    print(f\"----------------------------------------------------------------------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088e516",
   "metadata": {},
   "source": [
    "Run Entire Processes to get MobileNetV3_concrete_crack_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845cd6",
   "metadata": {},
   "source": [
    "Evaluate Model using Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18790efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "📊 Evaluation for TRAIN\n",
      "Accuracy: 0.9712989156058462\n",
      "Precision: 0.9859035119698627\n",
      "Recall: 0.9562706270627063\n",
      "F1-Score: 0.9708610064022019\n",
      "AUC-ROC: 0.9934172272804165\n",
      "Confusion Matrix: [[8368, 116], [371, 8113]]\n",
      "\n",
      "📊 Evaluation for VAL\n",
      "Accuracy: 0.8806608819683822\n",
      "Precision: 0.5601036269430052\n",
      "Recall: 0.8745954692556634\n",
      "F1-Score: 0.6828806064434618\n",
      "AUC-ROC: 0.9540645391404492\n",
      "Confusion Matrix: [[6328, 849], [155, 1081]]\n",
      "\n",
      "📊 Evaluation for TEST\n",
      "Accuracy: 0.8771241830065359\n",
      "Precision: 0.5652392947103274\n",
      "Recall: 0.8677494199535963\n",
      "F1-Score: 0.6845637583892618\n",
      "AUC-ROC: 0.9481623230785171\n",
      "Confusion Matrix: [[6259, 863], [171, 1122]]\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Training and evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "# Use the updated weights parameter instead of deprecated 'pretrained'\n",
    "weights = MobileNet_V3_Large_Weights.DEFAULT\n",
    "model = mobilenet_v3_large(weights=weights)\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, 2)  # 2 classes: cracked / not cracked\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(torch.load('/Users/kwinyarutpoungsangthanakul/Desktop/university/year3/MLOps/Structural-Defects-Network-MLOps/src/MobileNetV3_concrete_crack_detector.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Define the transform for test data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_on_split(split_name):\n",
    "    base_path = f\"../artifact_folder/{split_name}\"\n",
    "    labels_df = pd.read_csv(os.path.join(base_path, \"labels.csv\"))\n",
    "    images_dir = os.path.join(base_path, \"images\")\n",
    "\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        img_path = os.path.join(images_dir, row[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            prob = torch.softmax(output, dim=1)[0][1].item()\n",
    "\n",
    "        label = 1 if row[\"label\"].lower() == \"cracked\" else 0\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "        y_score.append(prob)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC-ROC\": roc_auc_score(y_true, y_score),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "# Run on all sets\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(f\"📊 Evaluation for {split.upper()}\")\n",
    "    metrics = evaluate_model_on_split(split)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print()\n",
    "print(f\"----------------------------------------------------------------------------\")\n",
    "print(\"Training and evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
