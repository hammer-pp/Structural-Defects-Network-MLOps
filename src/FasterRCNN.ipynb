{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e40fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d57c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteCrackDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, transforms=None, augment=False):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.augment = augment \n",
    "        self.split_dir = os.path.join(root_dir, split)\n",
    "        self.imgs = []\n",
    "\n",
    "        for root, _, files in os.walk(self.split_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.imgs.append(os.path.join(root, file))\n",
    "\n",
    "        print(f\"Found {len(self.imgs)} images in {split}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        img_dir, img_name = os.path.split(img_path)\n",
    "        annot_path = os.path.join(\n",
    "            img_dir,\n",
    "            img_name.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt')\n",
    "        )\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = img.size\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if os.path.exists(annot_path):\n",
    "            with open(annot_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    cls_id = int(parts[0])\n",
    "                    x_center = float(parts[1]) * width\n",
    "                    y_center = float(parts[2]) * height\n",
    "                    box_width = float(parts[3]) * width\n",
    "                    box_height = float(parts[4]) * height\n",
    "\n",
    "                    x1 = max(0, x_center - box_width / 2)\n",
    "                    y1 = max(0, y_center - box_height / 2)\n",
    "                    x2 = min(width, x_center + box_width / 2)\n",
    "                    y2 = min(height, y_center + box_height / 2)\n",
    "\n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(1)\n",
    "\n",
    "        # Optional: Apply horizontal flip before ToTensor\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "            flipped_boxes = []\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                new_x1 = width - x2\n",
    "                new_x2 = width - x1\n",
    "                flipped_boxes.append([new_x1, y1, new_x2, y2])\n",
    "            boxes = flipped_boxes\n",
    "\n",
    "        # Convert to tensor\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Target dict\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.zeros((0,), dtype=torch.float32),\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # Apply transforms (like ToTensor, Normalize, etc.)\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img,target)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02374338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for data augmentation and preprocessing\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = T.ToTensor()(image)  # Convert PIL image to tensor\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            image = F.hflip(image)  # Flip image horizontally\n",
    "\n",
    "            # Flip bounding boxes\n",
    "            if target[\"boxes\"].shape[0] > 0:\n",
    "                width = image.shape[2]  # (C, H, W)\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "        return image, target\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = T.Normalize(mean=self.mean, std=self.std)(image)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61237cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the Faster R-CNN model\n",
    "def get_faster_rcnn_model(num_classes=2):  # 2classes: background +crack\n",
    "    # Load a pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698cffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on validation set\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize evaluation metric\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = list(image.to(device) for image in images)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs and targets to format expected by metric\n",
    "            for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                pred = {\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                gt = {\n",
    "                    'boxes': target['boxes'].cpu(),\n",
    "                    'labels': target['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                # Update metric\n",
    "                metric.update([pred], [gt])\n",
    "    \n",
    "    # Compute metric\n",
    "    result = metric.compute()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3210508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train one epoch\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0 \n",
    "\n",
    "    # for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "    for batch in data_loader:\n",
    "\n",
    "        images, targets = batch  # unpack tuple\n",
    "        \n",
    "        # Ensure images are moved to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        # Move targets to device\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Filter out images with no boxes\n",
    "        valid_images, valid_targets = [], []\n",
    "        for img, tgt in zip(images, targets):\n",
    "            if tgt[\"boxes\"].numel() > 0:\n",
    "                valid_images.append(img)\n",
    "                valid_targets.append(tgt)\n",
    "        \n",
    "        if len(valid_targets) == 0:\n",
    "            continue  # skip if no annotations\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(valid_images, valid_targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    if num_batches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a1ba79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs):\n",
    "    best_map = 0.0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Train for one epoch\n",
    "        epoch_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        val_map = val_metrics['map'].item()\n",
    "        print(f\"Validation mAP: {val_map:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_map > best_map:\n",
    "            best_map = val_map\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            torch.save(best_model_wts, f'best_model_epoch_{epoch+1}.pth')\n",
    "            print(f\"Saved best model with mAP: {best_map:.4f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe16495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize predictions\n",
    "def visualize_prediction(model, img_path, device, threshold=0.5):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and transform the image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "    \n",
    "    # Convert image back to numpy for visualization\n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    # Draw bounding boxes on the image\n",
    "    for idx, box in enumerate(prediction[0]['boxes']):\n",
    "        score = prediction[0]['scores'][idx].item()\n",
    "        if score > threshold:\n",
    "            x1, y1, x2, y2 = box.cpu().numpy().astype(np.int32)\n",
    "            cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label = f\"Crack: {score:.2f}\"\n",
    "            cv2.putText(img_np, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294530c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate on test set\n",
    "def evaluate_test_set(model, test_loader, device, output_dir='test_results'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize evaluation metric\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            # Move images to device\n",
    "            images = list(image.to(device) for image in images)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs and targets for metric calculation\n",
    "            for j, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                pred = {\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                gt = {\n",
    "                    'boxes': target['boxes'].cpu(),\n",
    "                    'labels': target['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                # Update metric\n",
    "                metric.update([pred], [gt])\n",
    "            \n",
    "            # Save visualizations for the first few samples\n",
    "            if i < 10:  # Save first 10 test examples\n",
    "                img_np = np.array(images[0].cpu().permute(1, 2, 0))\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                \n",
    "                # Draw ground truth boxes in blue\n",
    "                for box, label in zip(targets[0]['boxes'], targets[0]['labels']):\n",
    "                    if label == 1:  # Only draw crack boxes\n",
    "                        x1, y1, x2, y2 = box.cpu().numpy().astype(np.int32)\n",
    "                        cv2.rectangle(img_np, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue for ground truth\n",
    "                \n",
    "                # Draw predicted boxes in green\n",
    "                for box, score, label in zip(outputs[0]['boxes'], outputs[0]['scores'], outputs[0]['labels']):\n",
    "                    if score > 0.5 and label == 1:  # Only draw high confidence crack predictions\n",
    "                        x1, y1, x2, y2 = box.cpu().numpy().astype(np.int32)\n",
    "                        cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green for predictions\n",
    "                        cv2.putText(img_np, f\"{score:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "                \n",
    "                # Save image\n",
    "                cv2.imwrite(os.path.join(output_dir, f\"test_result_{i}.jpg\"), cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    # Compute final metrics\n",
    "    result = metric.compute()\n",
    "    print(\"Test Results:\")\n",
    "    for k, v in result.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open(os.path.join(output_dir, \"test_metrics.txt\"), \"w\") as f:\n",
    "        for k, v in result.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977b27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collate function outside of main()\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the training and evaluation\n",
    "def main():\n",
    "        \n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    train_transform = Compose([\n",
    "        ToTensor(),\n",
    "        RandomHorizontalFlip(0.5),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    root_dir = '../artifact_folder'\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = ConcreteCrackDataset(\n",
    "        root_dir=root_dir,\n",
    "        split='train/images',\n",
    "        transforms=train_transform,\n",
    "        augment=True\n",
    "    )\n",
    "    val_dataset = ConcreteCrackDataset(\n",
    "        root_dir=root_dir,\n",
    "        split='val/images',\n",
    "        transforms=val_test_transform,\n",
    "        augment=True\n",
    "    )\n",
    "    test_dataset = ConcreteCrackDataset(\n",
    "        root_dir=root_dir,\n",
    "        split='test/images',  \n",
    "        transforms=val_test_transform,\n",
    "        augment=True\n",
    "    )\n",
    "        \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4, \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn \n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Get model\n",
    "    model = get_faster_rcnn_model(num_classes=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define optimizer and learning rate scheduler\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Use learning rate scheduler to reduce lr by 0.1 every 3 epochs\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    model, best_map = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        device, \n",
    "        num_epochs=15\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "    print(f\"Best validation mAP: {best_map:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'final_faster_rcnn_concrete_crack_detector.pth')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_results = evaluate_test_set(model, test_loader, device)\n",
    "    \n",
    "    print(\"Training and evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e27fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 16968 images in train/images\n",
      "Found 8413 images in val/images\n",
      "Found 8415 images in test/images\n",
      "Starting training...\n",
      "Epoch 1/15\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwinyarutpoungsangthanakul/Desktop/university/year3/MLOps/Structural-Defects-Network-MLOps/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4207/4207 [2:38:24<00:00,  2.26s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mAP: -1.0000\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|███████▉  | 3350/4207 [2:08:41<34:13,  2.40s/it]  "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
