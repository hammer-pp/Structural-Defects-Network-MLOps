{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24e40fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "535c6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Concrete Crack Dataset\n",
    "class ConcreteCrackDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory with all the data\n",
    "            split (string): 'train', 'val', or 'test' split\n",
    "            transforms (callable, optional): Optional transforms to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Set path to the specific split (train, val, or test)\n",
    "        self.split_dir = os.path.join(root_dir, split)\n",
    "        \n",
    "        # Get all images recursively from the split directory\n",
    "        self.imgs = []\n",
    "        \n",
    "        # Walk through all subdirectories (Decks, Pavements, Walls, etc.)\n",
    "        for root, _, files in os.walk(self.split_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.imgs.append(os.path.join(root, file))\n",
    "        \n",
    "        print(f\"Found {len(self.imgs)} images in {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        \n",
    "        # Derive annotation path - assuming annotation files are in same directory with .txt extension\n",
    "        img_dir, img_name = os.path.split(img_path)\n",
    "        annot_path = os.path.join(img_dir, img_name.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get image dimensions\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Initialize boxes and labels lists\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        # Check if annotation file exists (for crack images)\n",
    "        if os.path.exists(annot_path):\n",
    "            with open(annot_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    # Parse annotation (assuming YOLO format: class x_center y_center width height)\n",
    "                    parts = line.strip().split()\n",
    "                    cls_id = int(parts[0])  # Typically 0 for crack in YOLO format\n",
    "                    \n",
    "                    # Convert YOLO format to (x1, y1, x2, y2)\n",
    "                    x_center = float(parts[1]) * width\n",
    "                    y_center = float(parts[2]) * height\n",
    "                    box_width = float(parts[3]) * width\n",
    "                    box_height = float(parts[4]) * height\n",
    "                    \n",
    "                    x1 = max(0, x_center - box_width / 2)\n",
    "                    y1 = max(0, y_center - box_height / 2)\n",
    "                    x2 = min(width, x_center + box_width / 2)\n",
    "                    y2 = min(height, y_center + box_height / 2)\n",
    "                    \n",
    "                    # Only add valid boxes\n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(1)  # 1 for crack (0 is background in Faster R-CNN)\n",
    "        \n",
    "        # If no cracks (empty annotation or non-existent file), this is likely a non-crack image\n",
    "        # In Faster R-CNN training, we still need a target dictionary, but it can have empty boxes\n",
    "        \n",
    "        # Convert to tensor\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        \n",
    "        # Calculate area\n",
    "        if len(boxes) > 0:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            area = torch.zeros((0,), dtype=torch.float32)\n",
    "        target[\"area\"] = area\n",
    "        \n",
    "        # Suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02374338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for data augmentation and preprocessing\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = transforms.ToTensor()(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.size\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            # Flip boxes\n",
    "            if target[\"boxes\"].shape[0] > 0:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
    "                target[\"boxes\"] = boxes\n",
    "                \n",
    "        return image, target\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        image = transforms.Normalize(mean=self.mean, std=self.std)(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c61237cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the Faster R-CNN model\n",
    "def get_faster_rcnn_model(num_classes=2):  # 2 classes: background and crack\n",
    "    # Load a pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7120145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train one epoch\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0  # Only count batches that were actually used\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        # Move images and targets to device\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Filter valid targets (some images might not have annotations)\n",
    "        valid_images, valid_targets = [], []\n",
    "        for img, tgt in zip(images, targets):\n",
    "            if tgt[\"boxes\"].shape[0] > 0:\n",
    "                valid_images.append(img)\n",
    "                valid_targets.append(tgt)\n",
    "        \n",
    "        if len(valid_targets) == 0:\n",
    "            continue  # Skip if no valid targets\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(valid_images, valid_targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Prevent division by zero\n",
    "    if num_batches == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "698cffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to evaluate the model on validation set\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize evaluation metric\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = list(image.to(device) for image in images)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs and targets to format expected by metric\n",
    "            for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                pred = {\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                gt = {\n",
    "                    'boxes': target['boxes'].cpu(),\n",
    "                    'labels': target['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                # Update metric\n",
    "                metric.update([pred], [gt])\n",
    "    \n",
    "    # Compute metric\n",
    "    result = metric.compute()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a1ba79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10):\n",
    "    best_map = 0.0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Train for one epoch\n",
    "        epoch_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        val_map = val_metrics['map'].item()\n",
    "        print(f\"Validation mAP: {val_map:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_map > best_map:\n",
    "            best_map = val_map\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            torch.save(best_model_wts, f'best_model_epoch_{epoch+1}.pth')\n",
    "            print(f\"Saved best model with mAP: {best_map:.4f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5fe16495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_prediction(model, img_path, device, threshold=0.5):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and transform the image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "    \n",
    "    # Convert image back to numpy for visualization\n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    # Draw bounding boxes on the image\n",
    "    for idx, box in enumerate(prediction[0]['boxes']):\n",
    "        score = prediction[0]['scores'][idx].item()\n",
    "        if score > threshold:\n",
    "            x1, y1, x2, y2 = box.cpu().numpy().astype(np.int32)\n",
    "            cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label = f\"Crack: {score:.2f}\"\n",
    "            cv2.putText(img_np, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "294530c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to evaluate on test set\n",
    "def evaluate_test_set(model, test_loader, device, output_dir='test_results'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize evaluation metric\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            # Move images to device\n",
    "            images = list(image.to(device) for image in images)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs and targets for metric calculation\n",
    "            for j, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                pred = {\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                gt = {\n",
    "                    'boxes': target['boxes'].cpu(),\n",
    "                    'labels': target['labels'].cpu()\n",
    "                }\n",
    "                \n",
    "                # Update metric\n",
    "                metric.update([pred], [gt])\n",
    "            \n",
    "            # Save visualizations for the first few samples\n",
    "            if i < 10:  # Save first 10 test examples\n",
    "                img_np = np.array(images[0].cpu().permute(1, 2, 0))\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                \n",
    "                # Draw ground truth boxes in blue\n",
    "                for box, label in zip(targets[0]['boxes'], targets[0]['labels']):\n",
    "                    if label == 1:  # Only draw crack boxes\n",
    "                        x1, y1, x2, y2 = box.cpu().numpy().astype(np.int32)\n",
    "                        cv2.rectangle(img_np, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue for ground truth\n",
    "                \n",
    "                # Draw predicted boxes in green\n",
    "                for box, score, label in zip(outputs[0]['boxes'], outputs[0]['scores'], outputs[0]['labels']):\n",
    "                    if score > 0.5 and label == 1:  # Only draw high confidence crack predictions\n",
    "                        x1, y1, x2, y2 = box.cpu().numpy().astype(np.int32)\n",
    "                        cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green for predictions\n",
    "                        cv2.putText(img_np, f\"{score:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "                \n",
    "                # Save image\n",
    "                cv2.imwrite(os.path.join(output_dir, f\"test_result_{i}.jpg\"), cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    # Compute final metrics\n",
    "    result = metric.compute()\n",
    "    print(\"Test Results:\")\n",
    "    for k, v in result.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open(os.path.join(output_dir, \"test_metrics.txt\"), \"w\") as f:\n",
    "        for k, v in result.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "977b27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collate function outside of main()\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the training and evaluation\n",
    "def main():\n",
    "    import random\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    train_transform = Compose([\n",
    "        ToTensor(),\n",
    "        RandomHorizontalFlip(0.5),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    root_dir = '../artifact_folder'\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = ConcreteCrackDataset(\n",
    "        root_dir=root_dir,\n",
    "        split='train/images',  # Updated path to match your structure\n",
    "        transforms=train_transform\n",
    "    )\n",
    "\n",
    "    val_dataset = ConcreteCrackDataset(\n",
    "        root_dir=root_dir,\n",
    "        split='val/images',  # Updated path to match your structure\n",
    "        transforms=val_test_transform\n",
    "    )\n",
    "\n",
    "    test_dataset = ConcreteCrackDataset(\n",
    "        root_dir=root_dir,\n",
    "        split='test/images',  # Updated path to match your structure\n",
    "        transforms=val_test_transform\n",
    "    )\n",
    "        \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,  # Adjust based on your GPU memory\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn  # Required for variable size inputs\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Get model\n",
    "    model = get_faster_rcnn_model(num_classes=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define optimizer and learning rate scheduler\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Use learning rate scheduler to reduce lr by 0.1 every 3 epochs\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model, best_map = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        device, \n",
    "        num_epochs=15\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "    print(f\"Best validation mAP: {best_map:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'final_faster_rcnn_concrete_crack_detector.pth')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_results = evaluate_test_set(model, test_loader, device)\n",
    "    \n",
    "    print(\"Training and evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e27fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
