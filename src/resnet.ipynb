{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b19507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55eca456",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61585d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrackDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.labels_df = pd.read_csv(csv_path)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = {'Non-cracked': 0, 'Cracked': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.iloc[idx, 0]\n",
    "        label = self.label_map[self.labels_df.iloc[idx, 1]]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedad1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = CrackDataset('../artifact_folder/train/labels.csv', '../artifact_folder/train/images', transform)\n",
    "val_dataset = CrackDataset('../artifact_folder/val/labels.csv', '../artifact_folder/val/images', transform)\n",
    "test_dataset = CrackDataset('../artifact_folder/test/labels.csv', '../artifact_folder/test/images', transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee704cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanphattongsirisukool/Documents/GitHub/Structural-Defects-Network-MLOps/env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nanphattongsirisukool/Documents/GitHub/Structural-Defects-Network-MLOps/env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 classes: Cracked / Non-cracked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d459dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "434b2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Accuracy: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9ed2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    val_accuracy = correct / len(val_loader.dataset)\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75037da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10595.1421, Accuracy: 0.6364\n",
      "Epoch 2, Loss: 8369.4019, Accuracy: 0.7607\n",
      "Epoch 3, Loss: 7632.7484, Accuracy: 0.7905\n",
      "Epoch 4, Loss: 6988.4895, Accuracy: 0.8122\n",
      "Epoch 5, Loss: 6575.0189, Accuracy: 0.8294\n",
      "Epoch 6, Loss: 6196.2417, Accuracy: 0.8418\n",
      "Epoch 7, Loss: 5777.6937, Accuracy: 0.8535\n",
      "Epoch 8, Loss: 5029.1505, Accuracy: 0.8744\n",
      "Epoch 9, Loss: 4336.9726, Accuracy: 0.8923\n",
      "Epoch 10, Loss: 3418.7692, Accuracy: 0.9177\n",
      "Validation Accuracy: 0.8733\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
    "evaluate(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9ef4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/resnet_crack_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62ddbca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation for TRAIN\n",
      "Accuracy: 0.934052333804809\n",
      "Precision: 0.9719338715878508\n",
      "Recall: 0.8939179632248939\n",
      "F1-Score: 0.9312948977712285\n",
      "AUC-ROC: 0.9850882188643193\n",
      "Confusion Matrix: [[8265, 219], [900, 7584]]\n",
      "\n",
      "ðŸ“Š Evaluation for VAL\n",
      "Accuracy: 0.8732913348389397\n",
      "Precision: 0.5607424071991001\n",
      "Recall: 0.7776911076443058\n",
      "F1-Score: 0.6516339869281046\n",
      "AUC-ROC: 0.9078117100283506\n",
      "Confusion Matrix: [[6350, 781], [285, 997]]\n",
      "\n",
      "ðŸ“Š Evaluation for TEST\n",
      "Accuracy: 0.8680926916221033\n",
      "Precision: 0.5438100621820238\n",
      "Recall: 0.7604743083003953\n",
      "F1-Score: 0.6341463414634146\n",
      "AUC-ROC: 0.8975873296663811\n",
      "Confusion Matrix: [[6343, 807], [303, 962]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Consistent transform (same used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_on_split(split_name):\n",
    "    base_path = f\"../artifact_folder/{split_name}\"\n",
    "    labels_df = pd.read_csv(os.path.join(base_path, \"labels.csv\"))\n",
    "    images_dir = os.path.join(base_path, \"images\")\n",
    "\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        img_path = os.path.join(images_dir, row[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            prob = torch.softmax(output, dim=1)[0][1].item()\n",
    "\n",
    "        label = 1 if row[\"label\"].lower() == \"cracked\" else 0\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "        y_score.append(prob)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC-ROC\": roc_auc_score(y_true, y_score),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "# Run on all sets\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(f\"ðŸ“Š Evaluation for {split.upper()}\")\n",
    "    metrics = evaluate_model_on_split(split)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d1e03c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanphattongsirisukool/Documents/GitHub/Structural-Defects-Network-MLOps/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e826131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # Step the scheduler if provided\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ac0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3190 | Train Acc: 0.9336 | Val Loss: 0.5962 | Val Acc: 0.7551\n",
      "Epoch 2/10 | Train Loss: 0.3091 | Train Acc: 0.9381 | Val Loss: 0.4578 | Val Acc: 0.8475\n",
      "Epoch 3/10 | Train Loss: 0.3013 | Train Acc: 0.9439 | Val Loss: 0.5146 | Val Acc: 0.8432\n",
      "Epoch 4/10 | Train Loss: 0.2876 | Train Acc: 0.9529 | Val Loss: 0.4240 | Val Acc: 0.8695\n",
      "Epoch 5/10 | Train Loss: 0.2769 | Train Acc: 0.9587 | Val Loss: 0.4575 | Val Acc: 0.8590\n",
      "Epoch 6/10 | Train Loss: 0.2774 | Train Acc: 0.9570 | Val Loss: 0.3901 | Val Acc: 0.8906\n",
      "Epoch 7/10 | Train Loss: 0.2701 | Train Acc: 0.9603 | Val Loss: 0.3726 | Val Acc: 0.8985\n",
      "Epoch 8/10 | Train Loss: 0.2639 | Train Acc: 0.9645 | Val Loss: 0.4373 | Val Acc: 0.8669\n",
      "Epoch 9/10 | Train Loss: 0.2604 | Train Acc: 0.9673 | Val Loss: 0.5067 | Val Acc: 0.8191\n",
      "Epoch 10/10 | Train Loss: 0.2617 | Train Acc: 0.9658 | Val Loss: 0.5446 | Val Acc: 0.8041\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cd48c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/resnet_cos_crack_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b750b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation for TRAIN\n",
      "Accuracy: 0.9738920320603489\n",
      "Precision: 0.9624985620614287\n",
      "Recall: 0.9862093352192363\n",
      "F1-Score: 0.9742096990161262\n",
      "AUC-ROC: 0.9971925132501126\n",
      "Confusion Matrix: [[8158, 326], [117, 8367]]\n",
      "\n",
      "ðŸ“Š Evaluation for VAL\n",
      "Accuracy: 0.8041126827528824\n",
      "Precision: 0.4283476898981989\n",
      "Recall: 0.8533541341653667\n",
      "F1-Score: 0.570385818561001\n",
      "AUC-ROC: 0.9042903575629774\n",
      "Confusion Matrix: [[5671, 1460], [188, 1094]]\n",
      "\n",
      "ðŸ“Š Evaluation for TEST\n",
      "Accuracy: 0.8046345811051694\n",
      "Precision: 0.42518752467429927\n",
      "Recall: 0.8513833992094861\n",
      "F1-Score: 0.5671406003159558\n",
      "AUC-ROC: 0.9050721689377816\n",
      "Confusion Matrix: [[5694, 1456], [188, 1077]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Consistent transform (same used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_on_split(split_name):\n",
    "    base_path = f\"../artifact_folder/{split_name}\"\n",
    "    labels_df = pd.read_csv(os.path.join(base_path, \"labels.csv\"))\n",
    "    images_dir = os.path.join(base_path, \"images\")\n",
    "\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        img_path = os.path.join(images_dir, row[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            prob = torch.softmax(output, dim=1)[0][1].item()\n",
    "\n",
    "        label = 1 if row[\"label\"].lower() == \"cracked\" else 0\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "        y_score.append(prob)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC-ROC\": roc_auc_score(y_true, y_score),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "# Run on all sets\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(f\"ðŸ“Š Evaluation for {split.upper()}\")\n",
    "    metrics = evaluate_model_on_split(split)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22762294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                preds = outputs.argmax(1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)  # Optionally use val_loss or convert to custom scheduler\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33150b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state_dict\n",
    "model.load_state_dict(torch.load('../model/resnet_cos_crack_classifier.pth'))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c0a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftF1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "        targets = targets.float()\n",
    "\n",
    "        TP = (probs * targets).sum()\n",
    "        FP = (probs * (1 - targets)).sum()\n",
    "        FN = ((1 - probs) * targets).sum()\n",
    "\n",
    "        soft_f1 = 2 * TP / (2 * TP + FP + FN + 1e-8)\n",
    "        return 1 - soft_f1  # we want to minimize loss, so use 1 - F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c608e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                preds = outputs.argmax(1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)  # Optionally use val_loss or convert to custom scheduler\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac0e47a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanphattongsirisukool/Documents/GitHub/Structural-Defects-Network-MLOps/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.0759 | Train Acc: 0.9298 | Val Loss: 0.4339 | Val F1: 0.5826\n",
      "Epoch 2/10 | Train Loss: 0.0978 | Train Acc: 0.9058 | Val Loss: 0.4222 | Val F1: 0.6001\n",
      "Epoch 3/10 | Train Loss: 0.1043 | Train Acc: 0.9005 | Val Loss: 0.5128 | Val F1: 0.4996\n",
      "Epoch 4/10 | Train Loss: 0.1108 | Train Acc: 0.8922 | Val Loss: 0.4658 | Val F1: 0.5529\n",
      "Epoch 5/10 | Train Loss: 0.1154 | Train Acc: 0.8878 | Val Loss: 0.4495 | Val F1: 0.5659\n",
      "Epoch 6/10 | Train Loss: 0.0799 | Train Acc: 0.9240 | Val Loss: 0.4063 | Val F1: 0.6137\n",
      "Epoch 7/10 | Train Loss: 0.0554 | Train Acc: 0.9477 | Val Loss: 0.4320 | Val F1: 0.5847\n",
      "Epoch 8/10 | Train Loss: 0.0463 | Train Acc: 0.9567 | Val Loss: 0.4270 | Val F1: 0.5937\n",
      "Epoch 9/10 | Train Loss: 0.0364 | Train Acc: 0.9661 | Val Loss: 0.4643 | Val F1: 0.5515\n",
      "Epoch 10/10 | Train Loss: 0.0245 | Train Acc: 0.9781 | Val Loss: 0.3754 | Val F1: 0.6479\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = SoftF1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c03f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/resnet_SoftF1Loss_crack_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0015d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation for TRAIN\n",
      "Accuracy: 0.9892739273927392\n",
      "Precision: 0.9948736289938007\n",
      "Recall: 0.9836162187647336\n",
      "F1-Score: 0.9892128971076339\n",
      "AUC-ROC: 0.998387193487989\n",
      "Confusion Matrix: [[8441, 43], [139, 8345]]\n",
      "\n",
      "ðŸ“Š Evaluation for VAL\n",
      "Accuracy: 0.8722215618685368\n",
      "Precision: 0.5584415584415584\n",
      "Recall: 0.7714508580343213\n",
      "F1-Score: 0.647887323943662\n",
      "AUC-ROC: 0.896638482283086\n",
      "Confusion Matrix: [[6349, 782], [293, 989]]\n",
      "\n",
      "ðŸ“Š Evaluation for TEST\n",
      "Accuracy: 0.8762923351158646\n",
      "Precision: 0.5645905420991926\n",
      "Recall: 0.7739130434782608\n",
      "F1-Score: 0.6528842947649216\n",
      "AUC-ROC: 0.8967909560794937\n",
      "Confusion Matrix: [[6395, 755], [286, 979]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Consistent transform (same used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_on_split(split_name):\n",
    "    base_path = f\"../artifact_folder/{split_name}\"\n",
    "    labels_df = pd.read_csv(os.path.join(base_path, \"labels.csv\"))\n",
    "    images_dir = os.path.join(base_path, \"images\")\n",
    "\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        img_path = os.path.join(images_dir, row[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            prob = torch.softmax(output, dim=1)[0][1].item()\n",
    "\n",
    "        label = 1 if row[\"label\"].lower() == \"cracked\" else 0\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "        y_score.append(prob)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC-ROC\": roc_auc_score(y_true, y_score),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "# Run on all sets\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(f\"ðŸ“Š Evaluation for {split.upper()}\")\n",
    "    metrics = evaluate_model_on_split(split)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258b072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
